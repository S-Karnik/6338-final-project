{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "failing-product",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float32"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Gym\n",
    "import Random\n",
    "import Flux\n",
    "import Flux: Chain, Dense, relu, tanh, glorot_uniform\n",
    "import ProgressBars: ProgressBar\n",
    "using Plots\n",
    "using Statistics\n",
    "\n",
    "const F = Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-coffee",
   "metadata": {},
   "source": [
    "# Advantage Actor-Critic (A2C) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "historical-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentHistory"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct AgentHistory\n",
    "    nS::Int\n",
    "    nA::Int\n",
    "    γ::F\n",
    "    states::Array{F}\n",
    "    actions::Array{Int}\n",
    "    rewards::Array{F}\n",
    "end\n",
    "AgentHistory(nS, nA, γ) = AgentHistory(nS, nA, γ, zeros(0),zeros(Int, 0),zeros(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executed-attention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_discounted_returns (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_discounted_returns(rewards; γ=0.95)\n",
    "    N = length(rewards)\n",
    "    discount_exps = γ.^(1:N)\n",
    "    rev_discounted_rewards = reverse(discount_exps .* rewards)\n",
    "    cum_discounted_rewards = reverse(cumsum(rev_discounted_rewards, dims=1))\n",
    "    R = cum_discounted_rewards ./ discount_exps\n",
    "    return (R .- mean(R)) ./ (std(R) + F(1e-10)) #speeds up training a lot\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inappropriate-scottish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_action(probs)\n",
    "    probs = Array(probs)\n",
    "    cprobs = cumsum(probs, dims=1)\n",
    "    sampled = cprobs .> rand() \n",
    "    sampled_action = mapslices(argmax, sampled, dims=1)[1]\n",
    "    return sampled_action\n",
    "end\n",
    "\n",
    "sample_action(Vector(1:10)/55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "creative-slovakia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Any}:\n",
       " Dense(4, 1, relu)   \u001b[90m# 5 parameters\u001b[39m\n",
       " Dense(1, 3, relu)   \u001b[90m# 6 parameters\u001b[39m\n",
       " Dense(3, 4, relu)   \u001b[90m# 16 parameters\u001b[39m\n",
       " Dense(4, 2)         \u001b[90m# 10 parameters\u001b[39m\n",
       " Dense(4, 1)         \u001b[90m# 5 parameters\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_ac_weights(hidden_units; x_dim=4, y_dim=2)\n",
    "    layers = []\n",
    "    x = x_dim\n",
    "    for y in [hidden_units...]\n",
    "        push!(layers, Dense(x, y, relu; bias=true, init=glorot_uniform))\n",
    "        x = y\n",
    "    end\n",
    "    push!(layers, Dense(x, y_dim; bias=true, init=glorot_uniform)) # Prob actions\n",
    "    push!(layers, Dense(x, 1; bias=true, init=glorot_uniform)) # Value function\n",
    "    return layers\n",
    "end\n",
    "\n",
    "w_ = init_ac_weights([1,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "instant-monitoring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(w, x)\n",
    "    x = float.(x)\n",
    "    shared_output = x\n",
    "    shared_output = Chain(w[1:end-2]...)(x)\n",
    "    prob_act_pre_scale = w[end-1](shared_output)\n",
    "    value = w[end](shared_output)\n",
    "    return prob_act_pre_scale, value\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colonial-syria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ac_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2fn(x) = sum(x .* x)\n",
    "\n",
    "function ac_loss(w, agent_history; lam_par=1)\n",
    "    nS, nA = agent_history.nS, agent_history.nA\n",
    "    M = length(agent_history.states) ÷ nS\n",
    "    states = reshape(agent_history.states, nS, M)\n",
    "    R = compute_discounted_returns(agent_history.rewards, γ=agent_history.γ)\n",
    "    \n",
    "    p, V = predict(w, states)\n",
    "    V = vec(V)\n",
    "    A = R .- V   # advantage  \n",
    "    inds = agent_history.actions + nA * (0:M-1)\n",
    "    lsp = Flux.logsoftmax(p, dims=1)[inds] # lsp is a vector\n",
    "    return -sum(lsp .* A) + L2fn(A) * lam_par\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "latin-brazilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_a2c (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ReinforcementLearning\n",
    "import StableRNGs\n",
    "\n",
    "function main_a2c(env_fn;\n",
    "    nS = 4, \n",
    "    nA = 2,\n",
    "    hidden = [32, 16], # width inner layers\n",
    "    lr = 1e-3,\n",
    "    γ = 0.99, #discount rate\n",
    "    episodes = 3000,\n",
    "    render = true,\n",
    "    seed = 5,\n",
    "    infotime = 50\n",
    ")\n",
    "    seed > 0 && Random.seed!(seed)\n",
    "    env = env_fn(; continuous=false, rng = StableRNGs.StableRNG(hash(seed)))\n",
    "    ac_weights = init_ac_weights(hidden, x_dim=nS, y_dim=nA)\n",
    "    opt = Flux.ADAM(lr)\n",
    "    avgreward = 0\n",
    "    avgrewards = [0.0 for _ in 1:episodes]\n",
    "    for episode=1:episodes\n",
    "        ReinforcementLearning.RLBase.reset!(env)\n",
    "        episode_rewards = 0\n",
    "        history = AgentHistory(nS, nA, γ)\n",
    "        state = env.state\n",
    "        for t=1:2000\n",
    "            p, V = predict(ac_weights, state)\n",
    "            p = Flux.softmax(p, dims=1)\n",
    "            action = sample_action(p)\n",
    "            reward = ReinforcementLearning.RLBase.reward(env)\n",
    "            append!(history.states, float(state))\n",
    "            push!(history.actions, action)\n",
    "            push!(history.rewards, reward)\n",
    "            env(action)\n",
    "            state = env.state\n",
    "            episode_rewards += reward\n",
    "            env.done && break\n",
    "        end\n",
    "        avgreward = 0.1 * episode_rewards + avgreward * 0.9\n",
    "        avgrewards[episode] = avgreward\n",
    "        params_ac = Flux.params(ac_weights)\n",
    "#         println(params_ac)\n",
    "        grads = Flux.gradient(() -> ac_loss(ac_weights, history), params_ac)\n",
    "        Flux.update!(opt, params_ac, grads) \n",
    "    end\n",
    "\n",
    "    return ac_weights, avgrewards\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-shift",
   "metadata": {},
   "source": [
    "## Asynchronous Advantage Actor Critic (A3C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "objective-domain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main_a3c (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearning\n",
    "import StableRNGs\n",
    "N_THREADS = Threads.nthreads()\n",
    "\n",
    "function main_a3c(env_fn;\n",
    "    nS = 4, \n",
    "    nA = 2,\n",
    "    hidden = [32, 16], # width inner layers\n",
    "    lr = 1e-3,\n",
    "    γ = 0.99, #discount rate,\n",
    "    num_agents = 2,\n",
    "    time_step_update = 10,\n",
    "    episodes = 3000,\n",
    "    render = true,\n",
    "    seed = 5,\n",
    "    infotime = 50\n",
    ")\n",
    "    seed > 0 && Random.seed!(seed)\n",
    "    weights_lock = Threads.SpinLock()\n",
    "    shared_ac_weights = init_ac_weights(hidden, x_dim=nS, y_dim=nA)\n",
    "    prev_rewards = [0.0 for _ in 1:num_agents]\n",
    "    shared_avgrewards = [0.0 for _ in 1:episodes]\n",
    "    params_shared_ac_weights = Flux.params(shared_ac_weights)\n",
    "    opt = Flux.ADAM(lr)\n",
    "    Threads.@threads for agent_id in 1:num_agents\n",
    "        env_thread = env_fn(; continuous=false, rng = StableRNGs.StableRNG(hash(seed)))\n",
    "        ac_weights_thread = shared_ac_weights\n",
    "        params_thread_ac_weights = Flux.params(ac_weights_thread)\n",
    "        for episode=1:episodes\n",
    "            ac_weights_thread = shared_ac_weights\n",
    "            reset!(env_thread)\n",
    "            state = env_thread.state\n",
    "            history = AgentHistory(nS, nA, γ)\n",
    "            for t=1:2000\n",
    "                p, V = predict(ac_weights_thread, state)\n",
    "                p = Flux.softmax(p, dims=1)\n",
    "                action = sample_action(p)\n",
    "                append!(history.states, float(state))\n",
    "                reward = RLBase.reward(env_thread)\n",
    "                push!(history.actions, action)\n",
    "                push!(history.rewards, reward)\n",
    "                env_thread(action)\n",
    "                state = env_thread.state\n",
    "                env_thread.done && break\n",
    "            end\n",
    "            @views avgreward = 0.1 * sum(history.rewards) + prev_rewards[agent_id] * 0.9\n",
    "            prev_rewards[agent_id] = avgreward\n",
    "            grads = Flux.gradient(() -> ac_loss(ac_weights_thread, history), params_thread_ac_weights)\n",
    "            lock(weights_lock) do\n",
    "                Flux.update!(opt, params_shared_ac_weights, grads) \n",
    "                shared_avgrewards[episode] += avgreward\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    shared_avgrewards /= num_agents\n",
    "    return shared_ac_weights, shared_avgrewards\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "urban-rouge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Dense(4, 32, relu), Dense(32, 16, relu), Dense(16, 2), Dense(16, 1)], [3.225, 5.702500000000001, 7.107250000000001, 8.796525, 11.066872500000002, 12.960185250000002, 13.214166725000004, 13.142750052500002, 13.753475047250005, 13.903127542525004  …  198.3997963315569, 198.6598166984012, 198.7938350285611, 198.68945152570498, 193.79550637313446, 194.24095573582105, 194.91686016223892, 195.52517414601502, 196.07265673141353, 196.56539105827216])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using TimerOutputs\n",
    "import JSON\n",
    "\n",
    "num_runs = 5\n",
    "num_agents = N_THREADS\n",
    "num_episodes = 2000\n",
    "env_fn = ReinforcementLearning.CartPoleEnv\n",
    "env = env_fn()\n",
    "env_nS, env_nA = length(ReinforcementLearning.state(env)), length(ReinforcementLearning.action_space(env))\n",
    "# println(env_nS, env_nA )\n",
    "# ac_weights, a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)\n",
    "a3c_shared_ac_weights, a3c_avgrewards = main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "turned-webster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 4 entries:\n",
       "  \"4\"   => Any[3.225, 5.7025, 7.10725, 8.79653, 11.0669, 12.9602, 13.2142, 13.1…\n",
       "  \"2\"   => Any[1.8, 4.87, 6.633, 7.8197, 9.93773, 10.294, 11.3646, 11.5281, 13.…\n",
       "  \"a2c\" => Any[1.1, 2.29, 5.061, 5.9549, 7.05941, 9.75347, 10.5781, 14.9203, 16…\n",
       "  \"3\"   => Any[3.2, 7.58, 8.68867, 10.4865, 11.6045, 12.944, 13.383, 15.4113, 1…"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"cartpole_a3c_rewards.json\"\n",
    "data = Dict(\"$N_THREADS\"=>a3c_avgrewards)\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "reserved-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "statistical-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_timer!()\n",
    "to_cartpole = TimerOutput()\n",
    "@timeit to_cartpole \"a2c\" begin\n",
    "    for i in 1:num_runs\n",
    "        @timeit to_cartpole \"a2c_$i\" begin\n",
    "            main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@timeit to_cartpole \"a3c\" begin\n",
    "    for i in 1:num_runs\n",
    "        @timeit to_cartpole \"a3c_$i\" begin\n",
    "            main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "longer_ac_weights, longer_a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes * num_agents)\n",
    "@timeit to_cartpole \"a2c (same # grad steps as a3c)\" begin\n",
    "for i in 1:num_runs\n",
    "    @timeit to_cartpole \"a2c_$i\" begin\n",
    "        main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes * num_agents)\n",
    "    end\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "detected-niagara",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"2\" => Any[3.25, 6.275, 7.3475, 7.91275, 9.97147, 10.7743, 12.3969, 13.0072, …\n",
       "  \"3\" => Any[3.2, 7.58, 8.68867, 10.4865, 11.6045, 12.944, 13.383, 15.4113, 15.…"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = plot(a2c_avgrewards[50:end], xlabel=\"Episode\", ylabel=\"Episode Reward\", title=\"Episode Rewards of A2C and A3C on CartPoleEnv\", label=\"A2C\")\n",
    "plot!(p, a3c_avgrewards[50:end], label=\"A3C\")\n",
    "savefig(p, \"cartpole_env_performance_$N_THREADS.png\")\n",
    "file_name = \"cartpole_a3c_rewards.json\"\n",
    "data = Dict(\"$N_THREADS\"=>a3c_avgrewards)\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "vocal-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(file_name)\n",
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "retired-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
      "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
      "                               ───────────────────────   ────────────────────────\n",
      "       Tot / % measured:            3.53m /  42.8%           60.3GiB /  83.2%    \n",
      "\n",
      " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
      " ────────────────────────────────────────────────────────────────────────────────\n",
      " a2c (same # grad s...      1    40.1s   44.2%   40.1s   21.2GiB   42.3%  21.2GiB\n",
      "   a2c_5                    1    8.37s    9.2%   8.37s   4.25GiB    8.5%  4.25GiB\n",
      "   a2c_4                    1    8.10s    8.9%   8.10s   4.25GiB    8.5%  4.25GiB\n",
      "   a2c_3                    1    7.94s    8.8%   7.94s   4.25GiB    8.5%  4.25GiB\n",
      "   a2c_2                    1    7.92s    8.8%   7.92s   4.25GiB    8.5%  4.25GiB\n",
      "   a2c_1                    1    7.72s    8.5%   7.72s   4.25GiB    8.5%  4.25GiB\n",
      " a3c                        1    33.1s   36.5%   33.1s   19.4GiB   38.7%  19.4GiB\n",
      "   a3c_5                    1    6.98s    7.7%   6.98s   3.91GiB    7.8%  3.91GiB\n",
      "   a3c_4                    1    6.88s    7.6%   6.88s   3.89GiB    7.8%  3.89GiB\n",
      "   a3c_1                    1    6.49s    7.2%   6.49s   3.82GiB    7.6%  3.82GiB\n",
      "   a3c_3                    1    6.36s    7.0%   6.36s   3.93GiB    7.8%  3.93GiB\n",
      "   a3c_2                    1    6.35s    7.0%   6.35s   3.84GiB    7.7%  3.84GiB\n",
      " a2c                        1    17.4s   19.3%   17.4s   9.53GiB   19.0%  9.53GiB\n",
      "   a2c_2                    1    3.92s    4.3%   3.92s   1.91GiB    3.8%  1.91GiB\n",
      "   a2c_1                    1    3.87s    4.3%   3.87s   1.91GiB    3.8%  1.91GiB\n",
      "   a2c_3                    1    3.52s    3.9%   3.52s   1.91GiB    3.8%  1.91GiB\n",
      "   a2c_5                    1    3.17s    3.5%   3.17s   1.91GiB    3.8%  1.91GiB\n",
      "   a2c_4                    1    2.96s    3.3%   2.96s   1.91GiB    3.8%  1.91GiB\n",
      "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m"
     ]
    }
   ],
   "source": [
    "show(to_cartpole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "broadband-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 6 entries:\n",
       "  \"total_time_ns\"         => 0\n",
       "  \"total_allocated_bytes\" => 0\n",
       "  \"time_ns\"               => 6345415800\n",
       "  \"n_calls\"               => 1\n",
       "  \"allocated_bytes\"       => 4127536504\n",
       "  \"inner_timers\"          => Dict{String, Any}()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_cartpole_dict = TimerOutputs.todict(to_cartpole)\n",
    "to_cartpole_dict[\"inner_timers\"][\"a3c\"][\"inner_timers\"][\"a3c_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "flexible-advisory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "process_timer_env_allocated_bytes (generic function with 1 method)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_factor = 1_000_000_000\n",
    "function process_timer_env_times(timer_output)\n",
    "    compare_methods = collect(keys(timer_output[\"inner_timers\"]))\n",
    "    avg_times = Dict()\n",
    "    std_times = Dict()\n",
    "    for method in compare_methods\n",
    "        times = []\n",
    "        count = length(timer_output[\"inner_timers\"][method][\"inner_timers\"])\n",
    "        for sub_method in keys(timer_output[\"inner_timers\"][method][\"inner_timers\"])\n",
    "            push!(times, timer_output[\"inner_timers\"][method][\"inner_timers\"][sub_method][\"time_ns\"])\n",
    "        end\n",
    "        times /= 1_000_000_000\n",
    "        avg_times[method] = sum(times) / count\n",
    "        std_times[method] = std(times)\n",
    "    end\n",
    "    return avg_times, std_times\n",
    "end\n",
    "\n",
    "function process_timer_env_allocated_bytes(timer_output)\n",
    "    compare_methods = collect(keys(timer_output[\"inner_timers\"]))\n",
    "    avg_allocated_bytes = Dict()\n",
    "    std_allocated_bytes = Dict()\n",
    "    for method in compare_methods\n",
    "        allocated_bytes = []\n",
    "        count = length(timer_output[\"inner_timers\"][method][\"inner_timers\"])\n",
    "        for sub_method in keys(timer_output[\"inner_timers\"][method][\"inner_timers\"])\n",
    "            push!(allocated_bytes, timer_output[\"inner_timers\"][method][\"inner_timers\"][sub_method][\"allocated_bytes\"])\n",
    "        end\n",
    "        avg_allocated_bytes[method] = sum(allocated_bytes) / count\n",
    "        std_allocated_bytes[method] = std(allocated_bytes)\n",
    "    end\n",
    "    return avg_allocated_bytes, std_allocated_bytes\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "strong-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Measurements\n",
    "avg_times, std_times = process_timer_env_times(to_cartpole_dict)\n",
    "compare_methods = sort(collect(keys(avg_times)))\n",
    "p = bar(compare_methods, [avg_times[c] for c in compare_methods] .± [std_times[c] for c in compare_methods], legend = false, color=:blue)\n",
    "xlabel!(\"Comparison Method\")\n",
    "ylabel!(\"Time (s)\")\n",
    "title!(\"Comparison of A-C Methods for CartPoleEnv \\n Average Times Across $num_runs Runs\")\n",
    "savefig(p, \"cartpole_times_$N_THREADS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "spare-guinea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"4\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>11.17, \"a2c (same # …\n",
       "  \"2\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>6.68379, \"a2c (same …\n",
       "  \"3\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>9.67926, \"a2c (same …"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dict(\"$N_THREADS\"=>Dict(\"avg\"=>avg_times, \"std\"=>std_times))\n",
    "file_name = \"cartpole_ac_times.json\"\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "isolated-above",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Dict{Any, Any}} with 2 entries:\n",
       "  \"avg\" => Dict(\"a3c\"=>6.61215, \"a2c (same # grad steps as a3c)\"=>8.01112, \"a2c…\n",
       "  \"std\" => Dict(\"a3c\"=>0.298862, \"a2c (same # grad steps as a3c)\"=>0.243356, \"a…"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"2\"] = data[\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "declared-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm(file_name)\n",
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "lasting-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Measurements\n",
    "avg_allocated_bytes, std_allocated_bytes = process_timer_env_allocated_bytes(to_cartpole_dict)\n",
    "compare_methods = sort(collect(keys(avg_allocated_bytes)))\n",
    "p = bar(compare_methods, [avg_allocated_bytes[c] for c in compare_methods] .± [std_allocated_bytes[c] for c in compare_methods], legend = false, color=:green)\n",
    "xlabel!(\"Comparison Method\")\n",
    "ylabel!(\"Allocated Bytes\")\n",
    "title!(\"Comparison of A-C Methods for CartPoleEnv \\n Average Allocated Bytes Across $num_runs Runs\")\n",
    "savefig(p, \"cartpole_allocated_bytes_$N_THREADS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "derived-consultation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"4\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>9.22244e9, \"a2c (sam…\n",
       "  \"3\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>6.61042e9, \"a2c (sam…"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dict(\"$N_THREADS\"=>Dict(\"avg\"=>avg_allocated_bytes, \"std\"=>std_allocated_bytes))\n",
    "file_name = \"cartpole_ac_allocated_bytes.json\"\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "talented-newton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"4\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>9.22244e9, \"a2c (sam…\n",
       "  \"2\" => Dict{String, Dict{Any, Any}}(\"avg\"=>Dict(\"a3c\"=>4.1656e9, \"a2c (same #…\n",
       "  \"3\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>6.61042e9, \"a2c (sam…"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge(d, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ruled-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(file_name)\n",
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "invalid-flavor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.008782 seconds (25.70 M allocations: 2.410 GiB, 9.82% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-134.8604965442955, -259.2468208636801, -375.17787306121744, -490.00811125318296, -595.832073460638, -688.0052021290045, -762.4121980667677, -845.9464410677932, -915.1595682673668, -981.8307438259093  …  -1186.6136324452027, -1190.9090789883041, -1178.7435128935115, -1159.6201507299022, -1134.1148963938674, -1133.3722216948117, -1145.2777364187152, -1161.1367502120474, -1142.6404861702877, -1129.7534890616828])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time ac_weights, a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "appreciated-guyana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.080248 seconds (50.35 M allocations: 4.759 GiB, 13.80% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-139.623046875, -265.51470947265625, -382.36805419921876, -496.05235473632814, -595.3554322509766, -687.5835914184571, -758.8945560070802, -845.8646646153566, -912.0564635346803, -979.0677788511342  …  -1179.0233194972625, -1151.5637365709736, -1158.537917113095, -1146.0062079213167, -1141.6293755813335, -1146.8578198591376, -1143.8302104806457, -1163.3607270302373, -1156.0388816221357, -1153.7627827665626])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time a3c_shared_ac_weights, a3c_avgrewards = main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "controlled-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.141808 seconds (51.41 M allocations: 4.820 GiB, 8.82% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-134.8604965442955, -259.2468208636801, -375.17787306121744, -490.00811125318296, -595.832073460638, -688.0052021290045, -762.4121980667677, -845.9464410677932, -915.1595682673668, -981.8307438259093  …  -1066.996100366422, -1138.9548032753903, -1137.4490827458817, -1115.8315817487496, -1112.707022189823, -1098.623790708867, -1079.6536863026793, -1071.4806910941181, -1059.6452059683897, -1063.6791826504416])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time longer_ac_weights, longer_a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes * num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "laden-wallpaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_fn = PendulumEnv\n",
    "env = env_fn(; continuous=false)\n",
    "println(env.action)\n",
    "env_nS, env_nA = length(env.state), length(ReinforcementLearning.action_space(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "maritime-retailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.362033 seconds (25.70 M allocations: 2.410 GiB, 8.80% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-134.8604965442955, -259.2468208636801, -375.17787306121744, -490.00811125318296, -595.832073460638, -688.0052021290045, -762.4121980667677, -845.9464410677932, -915.1595682673668, -981.8307438259093  …  -1186.6136324452027, -1190.9090789883041, -1178.7435128935115, -1159.6201507299022, -1134.1148963938674, -1133.3722216948117, -1145.2777364187152, -1161.1367502120474, -1142.6404861702877, -1129.7534890616828])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time ac_weights, a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "convinced-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.560131 seconds (50.35 M allocations: 4.759 GiB, 13.44% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-137.77481079101562, -269.2098052978515, -385.9344485473633, -482.95477322387694, -581.2014345733643, -676.7445174343873, -751.7232761401673, -837.2632410066194, -909.0159879508793, -977.665976069854  …  -1145.1766640869653, -1122.9489085669406, -1138.0662308450123, -1123.8566902800424, -1112.2435371211786, -1097.125039732303, -1112.2856924973541, -1123.892731157775, -1104.7065647314507, -1101.1488171938527])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time a3c_shared_ac_weights, a3c_avgrewards = main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=2000, num_agents = num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "powered-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plot(a2c_avgrewards[50:end], xlabel=\"Episode\", ylabel=\"Episode Reward\", title=\"Episode Rewards of A2C and A3C on PendulumEnv\", label=\"A2C\")\n",
    "plot!(p, a3c_avgrewards[50:end], label=\"A3C\")\n",
    "savefig(p, \"pendulum_env_performance_$N_THREADS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "comfortable-context",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Any[Dense(2, 32, relu), Dense(32, 16, relu), Dense(16, 3), Dense(16, 1)], [-136.57911071777343, -261.5650564575195, -375.92269570922855, -488.86284496154786, -594.5558054605103, -689.7785910033265, -762.184975677408, -847.1223496916984, -911.9116436531926, -976.2697499177563  …  -1063.727435714301, -1042.9789139446286, -1061.476902677119, -1045.3984247507155, -1025.5608573610934, -1013.734964190902, -1022.2893211653666, -1040.362000376955, -1028.6181831517595, -1020.7341892384391])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_runs = 5\n",
    "num_agents = N_THREADS\n",
    "num_episodes = 2000\n",
    "env_fn = ReinforcementLearning.PendulumEnv\n",
    "env = env_fn(; continuous=false)\n",
    "env_nS, env_nA = length(env.state), length(ReinforcementLearning.action_space(env))\n",
    "# ac_weights, a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)\n",
    "a3c_shared_ac_weights, a3c_avgrewards = main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "optical-champagne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"2\"   => Any[-137.563, -274.916, -388.561, -488.61, -590.369, -684.806, -757.…\n",
       "  \"a2c\" => Any[-134.86, -259.247, -375.178, -490.008, -595.832, -688.005, -762.…\n",
       "  \"3\"   => Any[-138.143, -255.305, -375.771, -481.572, -588.936, -682.389, -754…"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = \"pendulum_a3c_rewards.json\"\n",
    "data = Dict(\"$N_THREADS\"=>a3c_avgrewards)\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "certified-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "virtual-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_timer!()\n",
    "to_pendulum = TimerOutput()\n",
    "num_agents = N_THREADS\n",
    "num_episodes = 2000\n",
    "env_fn = ReinforcementLearning.PendulumEnv\n",
    "env = env_fn(; continuous=false)\n",
    "env_nS, env_nA = length(env.state), length(ReinforcementLearning.action_space(env))\n",
    "\n",
    "ac_weights, a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)\n",
    "@timeit to_pendulum \"a2c\" begin\n",
    "    for i in 1:num_runs\n",
    "        @timeit to_pendulum \"a2c_$i\" begin\n",
    "            main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "a3c_shared_ac_weights, a3c_avgrewards = main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)\n",
    "@timeit to_pendulum \"a3c\" begin\n",
    "    for i in 1:num_runs\n",
    "        @timeit to_pendulum \"a3c_$i\" begin\n",
    "            main_a3c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes, num_agents = num_agents)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "longer_ac_weights, longer_a2c_avgrewards = main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes * num_agents)\n",
    "@timeit to_pendulum \"a2c (same # grad steps as a3c)\" begin\n",
    "for i in 1:num_runs\n",
    "    @timeit to_pendulum \"a2c_$i\" begin\n",
    "        main_a2c(env_fn; nS = env_nS, nA = env_nA, episodes=num_episodes * num_agents)\n",
    "    end\n",
    "end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cordless-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"pendulum_a3c_rewards.json\"\n",
    "data = Dict(\"$N_THREADS\"=>a3c_avgrewards)\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ceramic-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# println(merge(d, data))\n",
    "# rm(file_name)\n",
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "undefined-generator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
       "                               ───────────────────────   ────────────────────────\n",
       "       Tot / % measured:            3.39m /  53.0%           72.1GiB /  83.2%    \n",
       "\n",
       " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────────────────\n",
       " a2c (same # grad s...      1    44.7s   41.5%   44.7s   24.1GiB   40.2%  24.1GiB\n",
       "   a2c_2                    1    9.12s    8.5%   9.12s   4.82GiB    8.0%  4.82GiB\n",
       "   a2c_4                    1    9.03s    8.4%   9.03s   4.82GiB    8.0%  4.82GiB\n",
       "   a2c_1                    1    9.00s    8.4%   9.00s   4.82GiB    8.0%  4.82GiB\n",
       "   a2c_5                    1    8.89s    8.3%   8.89s   4.82GiB    8.0%  4.82GiB\n",
       "   a2c_3                    1    8.65s    8.0%   8.65s   4.82GiB    8.0%  4.82GiB\n",
       " a3c                        1    40.0s   37.1%   40.0s   23.8GiB   39.7%  23.8GiB\n",
       "   a3c_2                    1    8.62s    8.0%   8.62s   4.76GiB    7.9%  4.76GiB\n",
       "   a3c_4                    1    7.92s    7.4%   7.92s   4.76GiB    7.9%  4.76GiB\n",
       "   a3c_1                    1    7.90s    7.3%   7.90s   4.76GiB    7.9%  4.76GiB\n",
       "   a3c_3                    1    7.79s    7.2%   7.79s   4.76GiB    7.9%  4.76GiB\n",
       "   a3c_5                    1    7.77s    7.2%   7.77s   4.76GiB    7.9%  4.76GiB\n",
       " a2c                        1    23.0s   21.4%   23.0s   12.1GiB   20.1%  12.1GiB\n",
       "   a2c_4                    1    4.75s    4.4%   4.75s   2.41GiB    4.0%  2.41GiB\n",
       "   a2c_2                    1    4.69s    4.4%   4.69s   2.41GiB    4.0%  2.41GiB\n",
       "   a2c_3                    1    4.64s    4.3%   4.64s   2.41GiB    4.0%  2.41GiB\n",
       "   a2c_5                    1    4.49s    4.2%   4.49s   2.41GiB    4.0%  2.41GiB\n",
       "   a2c_1                    1    4.47s    4.2%   4.47s   2.41GiB    4.0%  2.41GiB\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "experimental-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 6 entries:\n",
       "  \"total_time_ns\"         => 107747770500\n",
       "  \"total_allocated_bytes\" => 64368402376\n",
       "  \"time_ns\"               => 0\n",
       "  \"n_calls\"               => 0\n",
       "  \"allocated_bytes\"       => 0\n",
       "  \"inner_timers\"          => Dict{String, Any}(\"a3c\"=>Dict{String, Any}(\"total_…"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pendulum_dict = TimerOutputs.todict(to_pendulum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "perfect-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Measurements\n",
    "avg_times, std_times = process_timer_env_times(to_pendulum_dict)\n",
    "compare_methods = sort(collect(keys(avg_times)))\n",
    "p = bar(compare_methods, [avg_times[c] for c in compare_methods] .± [std_times[c] for c in compare_methods], legend = false, color=:red)\n",
    "xlabel!(\"Comparison Method\")\n",
    "ylabel!(\"Time (s)\")\n",
    "title!(\"Comparison of A-C Methods for PendulumEnv \\n Average Times Across $num_runs Runs\")\n",
    "savefig(p, \"pendulum_times_$N_THREADS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cooked-antibody",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"4\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>12.2999, \"a2c (same …\n",
       "  \"3\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>10.7346, \"a2c (same …"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dict(\"$N_THREADS\"=>Dict(\"avg\"=>avg_times, \"std\"=>std_times))\n",
    "file_name = \"pendulum_ac_times.json\"\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "recorded-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(file_name)\n",
    "open(file_name, \"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "varying-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Measurements\n",
    "avg_allocated_bytes, std_allocated_bytes = process_timer_env_allocated_bytes(to_pendulum_dict)\n",
    "compare_methods = sort(collect(keys(avg_allocated_bytes)))\n",
    "p = bar(compare_methods, [avg_allocated_bytes[c] for c in compare_methods] .± [std_allocated_bytes[c] for c in compare_methods], legend = false, color=:orange)\n",
    "xlabel!(\"Comparison Method\")\n",
    "ylabel!(\"Allocated Bytes\")\n",
    "title!(\"Comparison of A-C Methods for PendulumEnv \\n Average Allocated Bytes Across $num_runs Runs\")\n",
    "savefig(p, \"pendulum_allocated_bytes_$N_THREADS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dedicated-highland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 3 entries:\n",
       "  \"4\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>1.02201e10, \"a2c (sa…\n",
       "  \"2\" => Dict{String, Dict{Any, Any}}(\"avg\"=>Dict(\"a3c\"=>5.11009e9, \"a2c (same …\n",
       "  \"3\" => Dict{String, Any}(\"avg\"=>Dict{String, Any}(\"a3c\"=>7.66511e9, \"a2c (sam…"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dict(\"$N_THREADS\"=>Dict(\"avg\"=>avg_allocated_bytes, \"std\"=>std_allocated_bytes))\n",
    "file_name = \"pendulum_allocated_bytes.json\"\n",
    "d = Dict()\n",
    "if isfile(file_name)\n",
    "    d = JSON.parsefile(file_name)\n",
    "end\n",
    "merge(d, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "deluxe-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(file_name)\n",
    "open(file_name,\"w\") do f\n",
    "    JSON.print(f, merge(d, data))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-identifier",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
